# ResNet18 ImageNet Configuration - DALI Backend
#
# This config uses NVIDIA DALI for GPU-accelerated data loading.
# DALI moves image decoding and augmentation to the GPU, significantly
# reducing CPU bottleneck compared to standard PyTorch DataLoader.
#
# Benefits over PyTorch DataLoader:
#   - GPU-accelerated JPEG decoding
#   - GPU-accelerated augmentations
#   - Lower CPU usage (important for systems with limited CPU cores)
#   - Better GPU utilization
#
# Requirements:
#   - nvidia-dali-cuda120 (conda install -c nvidia nvidia-dali-cuda120)
#   - ImageNet organized as train/val with class subdirectories
#
# Usage:
#   Single GPU: python scripts/train.py --config configs/resnet18_imagenet_dali.yaml
#   Multi-GPU:  torchrun --nproc_per_node=3 scripts/train.py --config configs/resnet18_imagenet_dali.yaml
#
# Fallback: If DALI is not installed, automatically falls back to PyTorch DataLoader.

experiment_name: resnet18_imagenet_dali

# Model configuration
model:
  name: resnet18
  num_classes: 1000

# Data configuration
data:
  dataset: imagenet
  root: "<IMAGENET_ROOT>"  # Auto-resolved from machine config
  img_size: 224

  # Backend selection: pytorch, dali, or cached
  backend: dali

  # DALI-specific settings
  dali:
    prefetch_queue_depth: 4    # Buffer depth for overlapping I/O and compute
    decoder_device: gpu        # "gpu" = nvJPEG + HW decoder (fast!)
                               # "cpu" = libjpeg-turbo (fallback only)

  # Note: DALI handles augmentations internally (RandomResizedCrop, HorizontalFlip, Normalize)
  # gpu_transforms is NOT used with DALI backend (DALI already runs on GPU)

# Training configuration
training:
  epochs: 40
  batch_size: 768              # Per-GPU batch size (A6000 48GB can handle this)
  lr: 0.3                     # Scaled for larger batch (linear scaling rule)
  min_lr: 0.0
  weight_decay: 1e-4
  optimizer: sgd
  momentum: 0.9
  scheduler: cosine
  label_smoothing: 0.1
  use_amp: true
  compile: true               # torch.compile for additional speedup
  sync_bn: false
  channels_last: true         # NHWC memory format for better GPU cache
  seed: 42

  # Speed optimizations
  val_frequency: 5            # Validate every N epochs
  save_frequency: 10          # Save checkpoint every N epochs
  disable_progress_bar: false

# Logging configuration
logging:
  log_dir: logs

# Weights & Biases experiment tracking
wandb:
  enabled: true
  project: visionCNN
  tags: []
